{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.2.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.1 (default, May 11 2017 13:04:09)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "filename = '/usr/local/Cellar/apache-spark/2.2.1/libexec/python/pyspark/shell.py'\n",
    "exec(open(filename).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"Feature\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let Spark know about the header and infer the Schema types!\n",
    "df_whole = spark.read.csv('../data/all_play.log.fn', sep = '\\t', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parsing data from date\n",
    "# https://stackoverflow.com/questions/46410887/pyspark-string-matching-to-create-new-column\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# use regulization expression for string manipulation \n",
    "# required feature 1 - date\n",
    "# e.g. \"20170301_play.log\" -> \"20170301\"\n",
    "df_whole = df_whole.withColumn('date', regexp_extract(col('file_name'), '([0-9]{8})(_)(\\w+)', 1))\n",
    "#df_whole.show()\n",
    "\n",
    "# required feature 2 - uid\n",
    "# e.g. \"154422682\" -> '154422682'(reserve the first night digits)\n",
    "df_whole = df_whole.withColumn('uid', regexp_extract(col('uid'), '([0-9]{9})', 1))\n",
    "#df_whole.show()\n",
    "\n",
    "# optional feature - play_time, song_length, song_id(maybe used for music recommendation)\n",
    "# select all there feature: date, uid, play_time, song_length, song_id\n",
    "# restore into a new dataframe\n",
    "df_select = df_whole.select(['date', 'uid', 'play_time', 'song_length', 'song_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop any row that contains missing data\n",
    "df_select = df_select.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the blank value\n",
    "df_select = df_select.filter(\"date != ''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace the error value: 20170339 -> 20170329\n",
    "from pyspark.sql.functions import *\n",
    "df_select_remove = df_select.withColumn('date', regexp_replace('date', '20170339', '20170329'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|date    |count  |\n",
      "+--------+-------+\n",
      "|20170301|3421492|\n",
      "|20170302|2452263|\n",
      "|20170303|1851942|\n",
      "|20170304|1709097|\n",
      "|20170305|1607932|\n",
      "|20170306|1351465|\n",
      "|20170307|1288366|\n",
      "|20170308|1230621|\n",
      "|20170309|1172860|\n",
      "|20170329|2193336|\n",
      "|20170330|4755802|\n",
      "|20170331|7033246|\n",
      "|20170401|5792550|\n",
      "|20170402|5699764|\n",
      "|20170403|3588991|\n",
      "|20170404|4941358|\n",
      "|20170405|3850905|\n",
      "|20170406|3881751|\n",
      "|20170407|3807564|\n",
      "|20170408|4053207|\n",
      "|20170409|3945463|\n",
      "|20170410|3435108|\n",
      "|20170411|2332928|\n",
      "|20170412|3457415|\n",
      "|20170413|3380796|\n",
      "|20170414|2278862|\n",
      "|20170415|3591673|\n",
      "|20170416|3558109|\n",
      "|20170417|3112159|\n",
      "|20170418|3013580|\n",
      "|20170419|3021366|\n",
      "|20170420|3008111|\n",
      "|20170421|3038619|\n",
      "|20170422|3247357|\n",
      "|20170423|3223963|\n",
      "|20170424|1506606|\n",
      "|20170425|2819081|\n",
      "|20170426|2789468|\n",
      "|20170427|3127340|\n",
      "|20170428|2952460|\n",
      "|20170429|3337713|\n",
      "|20170430|3153132|\n",
      "|20170501|2953738|\n",
      "|20170502|2486697|\n",
      "|20170503|2555248|\n",
      "|20170504|2516949|\n",
      "|20170505|2571197|\n",
      "|20170506|2874456|\n",
      "|20170507|2767560|\n",
      "|20170508|2513485|\n",
      "|20170509|2491675|\n",
      "|20170510|2490997|\n",
      "|20170511|2454415|\n",
      "|20170512|2522303|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the all possible type of date\n",
    "df_select_remove.groupBy('date').count().orderBy('date').show(90,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType, DateType\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# https://stackoverflow.com/questions/38080748/convert-pyspark-string-to-date-format\n",
    "\n",
    "# This function converts the string cell into a date:\n",
    "func =  udf (lambda x: datetime.strptime(x, '%Y%m%d'), DateType())\n",
    "\n",
    "df_select_use = df_select_remove.withColumn('date', func(col('date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-----------+--------+\n",
      "|      date|      uid|play_time|song_length| song_id|\n",
      "+----------+---------+---------+-----------+--------+\n",
      "|2017-03-30|168550892|      254|        254|23491655|\n",
      "|2017-03-30|168540455|      189|        190|  298250|\n",
      "|2017-03-30|168551247|       78|        149|11881432|\n",
      "|2017-03-30|168549788|       16|        242|  295469|\n",
      "|2017-03-30|168551248|       87|         87|21393368|\n",
      "|2017-03-30|168550496|      369|       2747|12495422|\n",
      "|2017-03-30|168551331|      231|        231|20671171|\n",
      "|2017-03-30|168535490|      283|        283| 6616004|\n",
      "|2017-03-30|168539760|      197|        198| 4732048|\n",
      "|2017-03-30|168551373|       14|        212| 3378911|\n",
      "|2017-03-30|168544926|        6|         28| 4403788|\n",
      "|2017-03-30|168551042|      106|        277|  505355|\n",
      "|2017-03-30|168551026|        2|         31|19477157|\n",
      "|2017-03-30|168532580|       67|        137|21762903|\n",
      "|2017-03-30|168551417|       27|        226|  727161|\n",
      "|2017-03-30|168551430|      231|        231| 1179220|\n",
      "|2017-03-30|168540348|    64528|          0|   77260|\n",
      "|2017-03-30|168550728|      282|        282|  285233|\n",
      "|2017-03-30|168551156|        0|        274|  313037|\n",
      "|2017-03-30|168530895|      264|        265|       0|\n",
      "+----------+---------+---------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select the start_date from 2017-03-30\n",
    "start_date = '2017-03-30'\n",
    "new_filtered_df_select_use = df_select_use.where(df_select_use.date >= start_date)\n",
    "new_filtered_df_select_use.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_filtered_df_select_use.write.csv('new_filtered_df_select_use.csv',header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
