{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00011.csv', '00012.csv', '00013.csv', '00014.csv', '00015.csv', '00016.csv', '00017.csv', '00018.csv', '00020.csv', '00021.csv', '00022.csv', '00023.csv', '00024.csv', '00025.csv', '00026.csv', '00027.csv', '00028.csv', '00029.csv', '00030.csv', '00031.csv', '00032.csv', '00033.csv', '00034.csv', '00035.csv', '00036.csv', '00037.csv', '00038.csv', '00039.csv', '00040.csv', '00041.csv', '00042.csv', '00043.csv', '00044.csv', '00045.csv', '00046.csv', '00047.csv', '00048.csv', '00049.csv', '00050.csv', '00051.csv', '00052.csv', '00053.csv', '00054.csv', '00055.csv', '00056.csv', '00057.csv', '00058.csv', '00059.csv', '00060.csv', '00061.csv', '00062.csv', '00063.csv', '00064.csv', '00065.csv', '00066.csv', '00067.csv', '00068.csv', '00069.csv', '00070.csv', '00071.csv', '00072.csv', '00073.csv', '00074.csv', '00075.csv', '00076.csv', '00077.csv', '00078.csv', '00079.csv', '00080.csv', '00081.csv', '00082.csv', '00083.csv', '00084.csv', '00085.csv', '00086.csv', '00087.csv', '00088.csv', '00089.csv', '00090.csv', '00091.csv', '00092.csv', '00093.csv', '00094.csv', '00095.csv', '00096.csv', '00097.csv', '00098.csv', '00099.csv', '00100.csv', '00101.csv', '00102.csv', '00103.csv', '00104.csv', '00105.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "directory = os.path.join(cwd + \"/clean_data/\")\n",
    "file_names = []\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_names.append(str(file))\n",
    "#print(file_names)\n",
    "\n",
    "def mid_3chars(x):\n",
    "    return(x[2:5])\n",
    "\n",
    "file_names_sorted = [filename for filename in sorted(file_names, key = mid_3chars)]\n",
    "print(file_names_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique user id in training set:529700\n",
      "the number of unique user id in training set:214323\n"
     ]
    }
   ],
   "source": [
    "# split the user id - training section and testing section\n",
    "import pandas as pd\n",
    "\n",
    "split_date = '2017-04-28'\n",
    "split_previous_user_id = []\n",
    "split_after_user_id = []\n",
    "for file_name_sorted in file_names_sorted:\n",
    "    df = pd.read_csv(directory + file_name_sorted)\n",
    "    split_previous_user_id = split_previous_user_id + df[(df.date <= split_date)].uid.tolist()\n",
    "    split_previous_user_id = list(set(split_previous_user_id))\n",
    "    split_after_user_id = split_after_user_id + df[(df.date > split_date)].uid.tolist()\n",
    "    split_after_user_id = list(set(split_after_user_id))\n",
    "\n",
    "print('the number of unique user id in training set:' + str(len(split_previous_user_id)))\n",
    "print('the number of unique user id in training set:' + str(len(split_after_user_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique user id only shown in training set:329901\n"
     ]
    }
   ],
   "source": [
    "# shown before the threshold but not shown after the threshold  \n",
    "print('the number of unique user id only shown in training set:' + str(len(list(set(split_previous_user_id) - set(split_after_user_id)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique user id only shown in testing set:14524\n"
     ]
    }
   ],
   "source": [
    "# shown after the threshold but not shown before the threshold  \n",
    "print('the number of unique user id only shown in testing set:' + str(len(list(set(split_after_user_id) - set(split_previous_user_id)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique user id shown in both training set and testing set:199799\n"
     ]
    }
   ],
   "source": [
    "# remove the uid shown after the threshold but not shown before the threshold \n",
    "split_after_remove_id = list(set(split_after_user_id) - set(split_previous_user_id))\n",
    "split_after_user_id_remove_new_user = list(set(split_after_user_id) - set(split_after_remove_id))\n",
    "print('the number of unique user id shown in both training set and testing set:' + str(len(split_after_user_id_remove_new_user)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of target label with not churn and churn = 0.3772 and 0.6228\n"
     ]
    }
   ],
   "source": [
    "# ratio of target - 1 (churn) versus 0 (not churn)\n",
    "# remove user id we don't care about\n",
    "terget_0_num = len(split_after_user_id_remove_new_user)\n",
    "terget_1_num = len(split_previous_user_id) - len(split_after_user_id_remove_new_user)\n",
    "print(\"The percentage of target label with not churn and churn = %.4f and %.4f\"%(terget_0_num/(terget_0_num + terget_1_num), terget_1_num/(terget_0_num + terget_1_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering \n",
    "\n",
    "Target Label: 1/0(churn / not churn) Down sampling to make it as a balanced problem\n",
    "\n",
    "Features:\n",
    "1. 1 day frequency on play log(# of record in playing log for each user id since 1 day before snapdate)\n",
    "2. 3 day frequency on play log(# of record in playing log for each user id since 3 day before snapdate) \n",
    "3. 7 day frequency on play log(# of record in playing log for each user id since 7 day before snapdate) \n",
    "4. 14 day frequency on play log(# of record in playing log for each user id since 14 day before snapdate)\n",
    "5. 30 day frequency on play log(# of record in playing log for each user id since 30 day before snapdate)\n",
    "6. Recency on play log\n",
    "7. completeness of listening activity (mean(total_play_time/total_song_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-27\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "snapdate = datetime.date(2017, 4 ,28)\n",
    "N = [1,3,7,14,30]\n",
    "date_N_days_ago = snapdate - datetime.timedelta(days=N[0])\n",
    "print(date_N_days_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 files are parsed!\n",
      "2 files are parsed!\n",
      "3 files are parsed!\n",
      "4 files are parsed!\n",
      "5 files are parsed!\n",
      "6 files are parsed!\n",
      "7 files are parsed!\n",
      "8 files are parsed!\n",
      "9 files are parsed!\n",
      "10 files are parsed!\n",
      "11 files are parsed!\n",
      "12 files are parsed!\n",
      "13 files are parsed!\n",
      "14 files are parsed!\n",
      "15 files are parsed!\n",
      "16 files are parsed!\n",
      "17 files are parsed!\n",
      "18 files are parsed!\n",
      "19 files are parsed!\n",
      "20 files are parsed!\n",
      "21 files are parsed!\n",
      "22 files are parsed!\n",
      "23 files are parsed!\n",
      "24 files are parsed!\n",
      "25 files are parsed!\n",
      "26 files are parsed!\n",
      "27 files are parsed!\n",
      "28 files are parsed!\n",
      "29 files are parsed!\n",
      "30 files are parsed!\n",
      "31 files are parsed!\n",
      "32 files are parsed!\n",
      "33 files are parsed!\n",
      "34 files are parsed!\n",
      "35 files are parsed!\n",
      "36 files are parsed!\n",
      "37 files are parsed!\n",
      "38 files are parsed!\n",
      "39 files are parsed!\n",
      "40 files are parsed!\n",
      "41 files are parsed!\n",
      "42 files are parsed!\n",
      "43 files are parsed!\n",
      "44 files are parsed!\n",
      "45 files are parsed!\n",
      "46 files are parsed!\n",
      "47 files are parsed!\n",
      "48 files are parsed!\n",
      "49 files are parsed!\n",
      "50 files are parsed!\n",
      "51 files are parsed!\n",
      "52 files are parsed!\n",
      "53 files are parsed!\n",
      "54 files are parsed!\n",
      "55 files are parsed!\n",
      "56 files are parsed!\n",
      "57 files are parsed!\n",
      "58 files are parsed!\n",
      "59 files are parsed!\n",
      "60 files are parsed!\n",
      "61 files are parsed!\n",
      "62 files are parsed!\n",
      "63 files are parsed!\n",
      "64 files are parsed!\n",
      "65 files are parsed!\n",
      "66 files are parsed!\n",
      "67 files are parsed!\n",
      "68 files are parsed!\n",
      "69 files are parsed!\n",
      "70 files are parsed!\n",
      "71 files are parsed!\n",
      "72 files are parsed!\n",
      "73 files are parsed!\n",
      "74 files are parsed!\n",
      "75 files are parsed!\n",
      "76 files are parsed!\n",
      "77 files are parsed!\n",
      "78 files are parsed!\n",
      "79 files are parsed!\n",
      "80 files are parsed!\n",
      "81 files are parsed!\n",
      "82 files are parsed!\n",
      "83 files are parsed!\n",
      "84 files are parsed!\n",
      "85 files are parsed!\n",
      "86 files are parsed!\n",
      "87 files are parsed!\n",
      "88 files are parsed!\n",
      "89 files are parsed!\n",
      "90 files are parsed!\n",
      "91 files are parsed!\n",
      "92 files are parsed!\n",
      "93 files are parsed!\n",
      "94 files are parsed!\n"
     ]
    }
   ],
   "source": [
    "# combine the each subfile into one\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_name_sorted in file_names_sorted:\n",
    "    df = pd.read_csv(directory + file_name_sorted, header=0, encoding='utf-8', index_col=False)\n",
    "    print(str(int(file_names_sorted.index(file_name_sorted)) + 1) + ' files are parsed!')\n",
    "    list_.append(df)\n",
    "frame = pd.concat(list_)\n",
    "whole_file = 'whole_clean_data.csv'\n",
    "frame.to_csv(directory + whole_file, encoding='utf-8', index=False, header = ['date', 'uid', 'play_time', 'song_length', 'song_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
