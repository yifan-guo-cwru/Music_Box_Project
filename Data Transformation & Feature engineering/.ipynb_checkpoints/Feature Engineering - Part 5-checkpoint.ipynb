{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00011.csv', '00012.csv', '00013.csv', '00014.csv', '00015.csv', '00016.csv', '00017.csv', '00018.csv', '00020.csv', '00021.csv', '00022.csv', '00023.csv', '00024.csv', '00025.csv', '00026.csv', '00027.csv', '00028.csv', '00029.csv', '00030.csv', '00031.csv', '00032.csv', '00033.csv', '00034.csv', '00035.csv', '00036.csv', '00037.csv', '00038.csv', '00039.csv', '00040.csv', '00041.csv', '00042.csv', '00043.csv', '00044.csv', '00045.csv', '00046.csv', '00047.csv', '00048.csv', '00049.csv', '00050.csv', '00051.csv', '00052.csv', '00053.csv', '00054.csv', '00055.csv', '00056.csv', '00057.csv', '00058.csv', '00059.csv', '00060.csv', '00061.csv', '00062.csv', '00063.csv', '00064.csv', '00065.csv', '00066.csv', '00067.csv', '00068.csv', '00069.csv', '00070.csv', '00071.csv', '00072.csv', '00073.csv', '00074.csv', '00075.csv', '00076.csv', '00077.csv', '00078.csv', '00079.csv', '00080.csv', '00081.csv', '00082.csv', '00083.csv', '00084.csv', '00085.csv', '00086.csv', '00087.csv', '00088.csv', '00089.csv', '00090.csv', '00091.csv', '00092.csv', '00093.csv', '00094.csv', '00095.csv', '00096.csv', '00097.csv', '00098.csv', '00099.csv', '00100.csv', '00101.csv', '00102.csv', '00103.csv', '00104.csv', '00105.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "directory = os.path.join(cwd + \"/clean_data/\")\n",
    "file_names = []\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_names.append(str(file))\n",
    "#print(file_names)\n",
    "\n",
    "def mid_3chars(x):\n",
    "    return(x[2:5])\n",
    "\n",
    "file_names_sorted = [filename for filename in sorted(file_names, key = mid_3chars)]\n",
    "print(file_names_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique user id in training set:529700\n",
      "the number of unique user id in training set:214323\n"
     ]
    }
   ],
   "source": [
    "# split the user id - training section and testing section\n",
    "import pandas as pd\n",
    "\n",
    "split_date = '2017-04-28'\n",
    "split_previous_user_id = []\n",
    "split_after_user_id = []\n",
    "for file_name_sorted in file_names_sorted:\n",
    "    df = pd.read_csv(directory + file_name_sorted)\n",
    "    split_previous_user_id = split_previous_user_id + df[(df.date <= split_date)].uid.tolist()\n",
    "    split_previous_user_id = list(set(split_previous_user_id))\n",
    "    split_after_user_id = split_after_user_id + df[(df.date > split_date)].uid.tolist()\n",
    "    split_after_user_id = list(set(split_after_user_id))\n",
    "\n",
    "print('the number of unique user id in training set:' + str(len(split_previous_user_id)))\n",
    "print('the number of unique user id in training set:' + str(len(split_after_user_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique user id only shown in training set:329901\n"
     ]
    }
   ],
   "source": [
    "# shown before the threshold but not shown after the threshold  \n",
    "print('the number of unique user id only shown in training set:' + str(len(list(set(split_previous_user_id) - set(split_after_user_id)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique user id only shown in testing set:14524\n"
     ]
    }
   ],
   "source": [
    "# shown after the threshold but not shown before the threshold  \n",
    "print('the number of unique user id only shown in testing set:' + str(len(list(set(split_after_user_id) - set(split_previous_user_id)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique user id shown in both training set and testing set:199799\n"
     ]
    }
   ],
   "source": [
    "# remove the uid shown after the threshold but not shown before the threshold \n",
    "split_after_remove_id = list(set(split_after_user_id) - set(split_previous_user_id))\n",
    "split_after_user_id_remove_new_user = list(set(split_after_user_id) - set(split_after_remove_id))\n",
    "print('the number of unique user id shown in both training set and testing set:' + str(len(split_after_user_id_remove_new_user)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of target label with not churn and churn = 0.3772 and 0.6228\n"
     ]
    }
   ],
   "source": [
    "# ratio of target - 1 (churn) versus 0 (not churn)\n",
    "# remove user id we don't care about\n",
    "terget_0_num = len(split_after_user_id_remove_new_user)\n",
    "terget_1_num = len(split_previous_user_id) - len(split_after_user_id_remove_new_user)\n",
    "print(\"The percentage of target label with not churn and churn = %.4f and %.4f\"%(terget_0_num/(terget_0_num + terget_1_num), terget_1_num/(terget_0_num + terget_1_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering \n",
    "\n",
    "Target Label: 1/0(churn / not churn) Down sampling to make it as a balanced problem\n",
    "\n",
    "Features:\n",
    "1. 1 day frequency on play log(# of record in playing log for each user id since 1 day before snapdate)\n",
    "2. 3 day frequency on play log(# of record in playing log for each user id since 3 day before snapdate) \n",
    "3. 7 day frequency on play log(# of record in playing log for each user id since 7 day before snapdate) \n",
    "4. 14 day frequency on play log(# of record in playing log for each user id since 14 day before snapdate)\n",
    "5. 30 day frequency on play log(# of record in playing log for each user id since 30 day before snapdate)\n",
    "6. Recency on play log\n",
    "7. completeness of listening activity (mean(total_play_time/total_song_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-27\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "snapdate = datetime.date(2017, 4 ,28)\n",
    "N = [1,3,7,14,30]\n",
    "date_N_days_ago = snapdate - datetime.timedelta(days=N[0])\n",
    "print(date_N_days_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine the each subfile into one\n",
    "path =r'C:\\DRO\\DCL_rawdata_files' # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_name_sorted in file_names_sorted[0:2]:\n",
    "    df = pd.read_csv(directory + file_name_sorted,index_col=None, header=None, encoding='utf-8', index=False)\n",
    "    list_.append(df)\n",
    "frame = pd.concat(list_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
